[
  {
    "objectID": "Our Model/Regression-Analysis.html",
    "href": "Our Model/Regression-Analysis.html",
    "title": "Predicting Asthma Prevalence by Census Tracts in Philadeplhia, PA",
    "section": "",
    "text": "# Regression Analysis & Cross-Validation\n\n\n\n\n\n\n\n\n\n\n\n\n\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\n\n\n\n## Plotting A Correlation Matrix\n\n\n\nCode\nregression_df = pd.read_csv('regression_df.csv')\n\n\n\n\nCode\nmodel = LinearRegression()\nmodel\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCode\nX = regression_df[['hw_value', 'oc_value', 'pmc_value', 'E_PARK', 'E_HOUAGE', 'smoking_prevalance','E_ROAD','blk_percent','white_percent','latino_percent', 'asian_percent', 'other_percent']].values\ny = regression_df['asthma_prevalance'].values\n\n\n\n\nCode\nfeature_cols = [col for col in regression_df.columns if col not in [\"asthma_prevalance\", \"geoid\"]]\ntrain_set, test_set = train_test_split(regression_df, test_size=0.3, random_state=42)\ny_train = train_set[\"asthma_prevalance\"].values\ny_test = test_set[\"asthma_prevalance\"].values\nX_train = train_set[feature_cols].values\nX_test = test_set[feature_cols].values\n\n\n\n\nCode\nsns.heatmap(\n    train_set[feature_cols].corr(), \n    cmap=\"coolwarm\", \n    annot=True, \n    vmin=-1, \n    vmax=1\n);\n\n\n\n\n\n\n\nCode\nX.shape\n\n\n(1179, 12)\n\n\n\n\nCode\ny.shape\n\n\n(1179,)\n\n\n\n\nCode\nmodel.fit(X, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\nCode\nRsq = model.score(X, y)\nRsq\n\n\n0.8813734475169395\n\n\n\n\nCode\nlinear_pipe = make_pipeline(StandardScaler(), LinearRegression())\n\nprint(\"Linear regression\")\nlinear_pipe.fit(X_train, y_train)\n\ntraining_score = linear_pipe.score(X_train, y_train)\nprint(f\"Training Score = {training_score}\")\n\ntest_score = linear_pipe.score(X_test, y_test)\nprint(f\"Test Score = {test_score}\")\n\n\nLinear regression\nTraining Score = 0.8771018545312711\nTest Score = 0.889628138385183\n\n\n\n\nCode\nforest_pipe = make_pipeline(\n    StandardScaler(),  # Pre-process step\n    RandomForestRegressor(n_estimators=100, max_depth=2, random_state=42),  # Model step\n)\n\nprint(\"Random forest\")\nforest_pipe.fit(X_train, y_train)\n\ntraining_score = forest_pipe.score(X_train, y_train)\nprint(f\"Training Score = {training_score}\")\n\ntest_score = forest_pipe.score(X_test, y_test)\nprint(f\"Test Score = {test_score}\")\n\n\nRandom forest\nTraining Score = 0.802226720882675\nTest Score = 0.7982253593098484\n\n\n\n\nCode\nforest_pipe.named_steps\nforest_model = forest_pipe['randomforestregressor']\n\n\n\n\nCode\nforest_model.feature_importances_\n\n\narray([0.13976872, 0.        , 0.        , 0.3803543 , 0.        ,\n       0.        , 0.        , 0.04474563, 0.43513135, 0.        ,\n       0.        , 0.        ])\n\n\n\n\nCode\nimportance = pd.DataFrame(\n    {\"Feature\": feature_cols, \"Importance\": forest_model.feature_importances_}\n).sort_values(\"Importance\", ascending=False)\n\n\n\n\nCode\nimportance\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n8\nwhite_percent\n0.435131\n\n\n3\nsmoking_prevalance\n0.380354\n\n\n0\nhw_value\n0.139769\n\n\n7\nblk_percent\n0.044746\n\n\n1\noc_value\n0.000000\n\n\n2\npmc_value\n0.000000\n\n\n4\nE_PARK\n0.000000\n\n\n5\nE_HOUAGE\n0.000000\n\n\n6\nE_ROAD\n0.000000\n\n\n9\nlatino_percent\n0.000000\n\n\n10\nasian_percent\n0.000000\n\n\n11\nother_percent\n0.000000\n\n\n\n\n\n\n\n\n\nCode\nimport hvplot.pandas\nimportance.sort_values(\"Importance\", ascending=True).hvplot.barh(\n    x=\"Feature\", y=\"Importance\", title=\"Factors that Impact Asthma Prevalance\"\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\nmodel = linear_pipe['linearregression']\n\n\n\n\nCode\nlinear_pipe = make_pipeline(StandardScaler(), LinearRegression())\n\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    linear_pipe,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.87672721 0.86979481 0.86632107]\nScores mean =  0.8709476945361351\nScore std dev =  0.004325797875727253\n\n\n\n\nCode\nforest_pipe = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    forest_pipe,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.8692119  0.91654952 0.9188434 ]\nScores mean =  0.9015349385674881\nScore std dev =  0.022875019221878082\n\n\n\n\nCode\npipe = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=42))\npipe\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\npipe.named_steps\n\n\n{'standardscaler': StandardScaler(),\n 'randomforestregressor': RandomForestRegressor(random_state=42)}\n\n\n\n\nCode\nmodel_step = \"randomforestregressor\"\nparam_grid = {\n    f\"{model_step}__n_estimators\": [5, 10, 15, 20, 30, 50, 100, 200],\n    f\"{model_step}__max_depth\": [2, 5, 7, 9, 13, 21, 33, 51],\n}\n\nparam_grid\n\n\n{'randomforestregressor__n_estimators': [5, 10, 15, 20, 30, 50, 100, 200],\n 'randomforestregressor__max_depth': [2, 5, 7, 9, 13, 21, 33, 51]}\n\n\n\n\nCode\ngrid = GridSearchCV(pipe, param_grid, cv=3, verbose=1)\ngrid.fit(X_train, y_train)\n\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\ndef evaluate_mape(model, X_test, y_test):\n    \"\"\"\n    Given a model and test features/targets, print out the \n    mean absolute error and accuracy\n    \"\"\"\n    # Make the predictions\n    predictions = model.predict(X_test)\n\n    # Absolute error\n    errors = abs(predictions - y_test)\n    avg_error = np.mean(errors)\n\n    # Mean absolute percentage error\n    mape = 100 * np.mean(errors / y_test)\n\n    # Accuracy\n    accuracy = 100 - mape\n\n    print(\"Model Performance\")\n    print(f\"Average Absolute Error: {avg_error:0.4f}\")\n    print(f\"Accuracy = {accuracy:0.2f}%.\")\n\n    return accuracy\n\n\n\n\nCode\nbase_model = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=42))\n\n# Fit the training set\nbase_model.fit(X_train, y_train)\n\n# Evaluate on the test set\nbase_accuracy = evaluate_mape(base_model, X_test, y_test)\n\n\nModel Performance\nAverage Absolute Error: 0.2733\nAccuracy = 97.68%.\n\n\n\n\nCode\ndata = regression_df.loc[test_set.index]\n\n\n\ncensustracts = gpd.read_file('Census_Tracts_2010.geojson')\ntract_and_geoid = censustracts[['GEOID10', 'TRACTCE10']]\ntract_and_geoid = tract_and_geoid.rename(columns = {'TRACTCE10':'tract'})\n\ncensustracts = censustracts[['GEOID10', 'geometry']]\ncensustracts = censustracts.rename(columns = {'GEOID10':'geoid'})\ncensustracts['geoid'] = censustracts['geoid'].astype(np.int64)\n\n\n\nCode\ndata['prediction'] = base_model.predict(X_test)\ndata.to_csv('data.csv', index=False)\n\n\n\n\nCode\ndata = pd.merge(data, censustracts, on='geoid', how ='inner')\n\n\n\n\nCode\ndata = gpd.GeoDataFrame(data, geometry = 'geometry')\n\n\n\n\nCode\nfig, axs = plt.subplots(ncols=2, figsize=(10, 10))\n\n# Predicted values\ndata.plot(\n    ax=axs[0],\n    column='prediction',\n    legend=True,\n    cmap='viridis',\n    linewidth=0.8,\n    edgecolor='0.8',\n    legend_kwds={'label': \"Predicted Asthma Prevalence\", 'orientation': \"horizontal\", 'shrink': 0.8}\n)\naxs[0].set_title(\"Predicted Asthma Prevalence\")\n\n# Actual values\ndata.plot(\n    ax=axs[1],\n    column='asthma_prevalance',\n    legend=True,\n    cmap='viridis',\n    linewidth=0.8,\n    edgecolor='0.8',\n    legend_kwds={'label': \"Actual Asthma Prevalence\", 'orientation': \"horizontal\", 'shrink': 0.8}\n)\naxs[1].set_title(\"Actual Asthma Prevalence\")\n\naxs[0].set_axis_off()\naxs[1].set_axis_off()\n\nplt.show()\n\n\n\n\n\n\n\nCode\n\n\n\nTypeError: Cannot interpret '&lt;geopandas.array.GeometryDtype object at 0x00000286BF603E80&gt;' as a data type"
  },
  {
    "objectID": "Our Model/index.html",
    "href": "Our Model/index.html",
    "title": "Analysis",
    "section": "",
    "text": "Analysis\nThis section includes examples of technical analysis done using Jupyter notebooks. Each sub-section highlights different types of analyses and visualizations. In particular, it highlights that we can easily publish interactive visualizations produced with packages such as hvPlot, altair, or Folium, without losing any of the interactive features.\nOn this page, you might want to share more introductory or background information about the analyses to help guide the reader."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#welcome",
    "href": "index.html#welcome",
    "title": "MUSA 550 Final Project Template",
    "section": "",
    "text": "We can create beautiful websites that describe complex technical analyses in Python using Quarto and deploy them online using GitHub Pages. This combination of tools is a really powerful way to create and share your work. This website is a demo that is meant to be used to create your own Quarto website for the final project in MUSA 550.\nQuarto is a relatively new tool, but is becoming popular quickly. It’s a successor to the Rmarkdown ecosystem that combines functionality into a single tool and also extends its computation power to other languages. Most importantly for us, Quarto supports executing Python code, allowing us to convert Jupyter notebooks to HTML and share them online.\n\n\n\n\n\n\nImportant\n\n\n\nThis template site, including the layout it uses, is just a suggested place to start! For your final project, you’re welcome (and encouraged) to make as many changes as you like to best fit your project."
  },
  {
    "objectID": "index.html#find-out-more",
    "href": "index.html#find-out-more",
    "title": "MUSA 550 Final Project Template",
    "section": "Find out more",
    "text": "Find out more\nThe code for this repository is hosted on our course’s GitHub page: https://github.com/MUSA-550-Fall-2023/quarto-website-template.\nWe covered the basics of getting started with Quarto and GitHub Pages in week 9. Take a look at the slides for lecture 9A to find out more."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "On this about page, you might want to add more information about yourself, the project, or course. Any helpful context could go here!\nMy name is Nick Hand, the instructor for the course. You can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2023.\nAdipisicing proident minim non non dolor quis. Pariatur in ipsum aliquip magna. Qui ad aliqua nulla excepteur dolor nostrud quis nisi. Occaecat proident eiusmod in cupidatat. Elit qui laboris sit aliquip proident dolore. Officia commodo commodo in eiusmod aliqua sint cupidatat consectetur aliqua sint reprehenderit.\nOccaecat incididunt esse et elit adipisicing sit est cupidatat consequat. Incididunt exercitation amet dolor non sit anim veniam veniam sint velit. Labore irure reprehenderit ut esse. Minim quis commodo nisi voluptate."
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html",
    "href": "Our Model/1-Exploratory-Analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "C:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#philadelphia-social-progress-index-data",
    "href": "Our Model/1-Exploratory-Analysis.html#philadelphia-social-progress-index-data",
    "title": "Exploratory Analysis",
    "section": "Philadelphia Social Progress Index Data",
    "text": "Philadelphia Social Progress Index Data\nAs social determinants of health influence the overall health of individuals, including asthma rates Philadelphia’s Social Progress Index, which measures social progress using a detailed framework of indicators applied in 372 census tracts, helps account for indicators related to conditions of environments that Philadelphians grew up in [5]. This analysis utilizes Health and Wellness data, represented as hw_value, to understand the overall health of communities, as more Black and Hispanic residents reported poor or fair health than any other group, with severe declines in life expectancy [6]. Philadelphia ranked among the worst 25 in the nation for ozone and particle pollution in the country, which are related to serious health effects such as asthma attacks. Further, particle pollution has been linked to the development of asthma [7]. To increase the accuracy of our prediction model, we used Ozone and Particulate Matter 2.5 data, represented as oc_value and pmc_value, respectively.\n\n\nCode\nhealth_and_wellness = pd.read_csv(\"health_and_wellness.csv\")\nhealth_and_wellness = health_and_wellness.drop(['rank', 'region_name', 'tract_name', 'variable', 'neighborhood_name', 'average_label'], axis=1)\nhealth_and_wellness = health_and_wellness.rename(columns={'value':'hw_value'})\n\n\n\n\nCode\nozone_concentration = pd.read_csv(\"ozone_concentration.csv\")\nozone_concentration = ozone_concentration.drop(['rank', 'region_name', 'tract_name', 'variable','neighborhood_name', 'average_label'], axis=1)\nozone_concentration = ozone_concentration.rename(columns={'value':'oc_value'})\n\n\n\n\nCode\npmc = pd.read_csv(\"particular_matter_concentration.csv\")\npmc = pmc.drop(['rank', 'region_name', 'tract_name', 'variable','neighborhood_name', 'average_label'], axis=1)\npmc = pmc.rename(columns={'value':'pmc_value'})"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#cdc-places-data",
    "href": "Our Model/1-Exploratory-Analysis.html#cdc-places-data",
    "title": "Exploratory Analysis",
    "section": "CDC PLACES Data",
    "text": "CDC PLACES Data\nThis analysis builds off of the CDC’s PLACES dataset, a model-based population-level analysis and community estimates of health measures across the U.S [4]. Our model uses asthma prevalence data, represented as asthma_prevalance, from this dataset as our dependent variable. Tobacco Smoke, represented through the smoking prevalence data, smoking_prevalance, is known to be an indoor and outdoor pollution source that triggers asthma. In the City of Philadelphia, while the rates are declining, Philadelphians continue to have the highest smoking rate among large U.S cities, with non-Hispanic Black and Hispanic residents more likely to report smoking [6].\n\n\nCode\nCDC_data = pd.read_csv(\"CDC_data.csv\")\nCDC_data = CDC_data[(CDC_data['StateAbbr'] == 'PA') &\n                         (CDC_data['StateDesc'] == 'Pennsylvania') &\n                         (CDC_data['CountyName'] == 'Philadelphia')]\nCDC_data = CDC_data[['CASTHMA_CrudePrev', 'TractFIPS', 'CSMOKING_CrudePrev']]\nCDC_data = CDC_data.rename(columns={'TractFIPS':'geoid'})\nCDC_data = CDC_data.rename(columns={'CASTHMA_CrudePrev':'asthma_prevalance'})\nCDC_data = CDC_data.rename(columns={'CSMOKING_CrudePrev': 'smoking_prevalance'})"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#environmental-justice-index-data",
    "href": "Our Model/1-Exploratory-Analysis.html#environmental-justice-index-data",
    "title": "Exploratory Analysis",
    "section": "Environmental Justice Index Data",
    "text": "Environmental Justice Index Data\nThis analysis uses data from the Environmental Justice Index, a place-based tool that is designed to measure cumulative impacts of environmental burden that affects health and health equity [3]. In Philadelphia, children are largely exposed to lead, mold and tobacco smoke in the homes of Philadelphia’s residents, which are common asthma triggers [8]. The predictive model reflects these indoor exposures, by including the percentage of houses built pre-1980, as nearly 90 percent of homes were built before 1978, represented as e_houage[8]. The Center of Excellence in Environmental Toxicology shares that children living close to busy roadways is also a major concern for asthma, justifying the data for the proportion of tract’s area within 1 mile buffer of a high-volume road or highway, e_road [8]. Green spaces remain a protective factor that improves air quality, including ozone and particle pollution, and increases the health of individuals, demonstrated in as a predictor variable, e_park, that provides the proportion of the tract’s area within a 1 mile buffer of greenspace[9].\n\n\nCode\neji_pa = pd.read_csv(\"eji_pa.csv\")\neji_pa = eji_pa[(eji_pa['COUNTY'] == 'Philadelphia')]\neji_pa = eji_pa[['GEOID', 'E_PARK', 'E_HOUAGE','E_ROAD']]\neji_pa = eji_pa.rename(columns={'GEOID': 'geoid'})\n\n\n\n\nCode\ncensustracts = gpd.read_file('Census_Tracts_2010.geojson')\ntract_and_geoid = censustracts[['GEOID10', 'TRACTCE10']]\ntract_and_geoid = tract_and_geoid.rename(columns = {'TRACTCE10':'tract'})\ncensustracts = censustracts[['GEOID10', 'geometry']]\ncensustracts = censustracts.rename(columns = {'GEOID10':'geoid'})\ncensustracts['geoid'] = censustracts['geoid'].astype(np.int64)"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#philly-demographics",
    "href": "Our Model/1-Exploratory-Analysis.html#philly-demographics",
    "title": "Exploratory Analysis",
    "section": "Philly Demographics",
    "text": "Philly Demographics\nAs Black and Latino individuals encounter higher asthma burden, our prediction model uses the Census Bureau to include demographic data including: the percentage of Black, Latino, Asian and other races and ethnicities in Philadelphia. This is represented as blk_percent, white_percent, latino_percent, asian_percent, and other_percent.\n\n\nCode\nvariables = [\n    \"NAME\",\n    \"B03002_001E\",\n    \"B03002_003E\", \n    \"B03002_004E\", \n    \"B03002_005E\", \n    \"B03002_006E\", \n    \"B03002_007E\", \n    \"B03002_008E\", \n    \"B03002_009E\", \n    \"B03002_012E\", \n]\n\n\n\n\nCode\navailable = cenpy.explorer.available()\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nphilly_county_code = \"101\"\npa_state_code = \"42\"\n\n\n\n\nCode\nphilly_demographics = acs.query(\n    cols=variables,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": pa_state_code, \"county\": philly_county_code, \"tract\": \"*\"},\n)\n\nphilly_demographics = philly_demographics.rename(\n    columns={\n        \"B03002_001E\": \"Total\", \n        \"B03002_003E\": \"White\",  \n        \"B03002_004E\": \"Black\",  \n        \"B03002_005E\": \"AI/AN\", \n        \"B03002_006E\": \"Asian\",  \n        \"B03002_007E\": \"NH/PI\", \n        \"B03002_008E\": \"Other_\",  \n        \"B03002_009E\": \"Two Plus\",\n        \"B03002_012E\": \"Hispanic\",\n    }\n)\nphilly_demographics = pd.merge(philly_demographics, tract_and_geoid, on='tract', how='inner')\nphilly_demographics = philly_demographics.rename(columns = {'GEOID10':'geoid'})\n\n\n\n\nCode\nphilly_demographics['Black'] = pd.to_numeric(philly_demographics['Black'])\nphilly_demographics['Total'] = pd.to_numeric(philly_demographics['Total'])\nphilly_demographics['blk_percent'] = (philly_demographics['Black'] / philly_demographics['Total']) * 100\n\n\n\n\nCode\nphilly_demographics['White'] = pd.to_numeric(philly_demographics['White'])\nphilly_demographics['white_percent'] = (philly_demographics['White'] / philly_demographics['Total']) * 100\nphilly_demographics['Hispanic'] = pd.to_numeric(philly_demographics['Hispanic'])\nphilly_demographics['latino_percent'] = (philly_demographics['Hispanic'] / philly_demographics['Total']) * 100\nphilly_demographics['Asian'] = pd.to_numeric(philly_demographics['Asian'])\nphilly_demographics['asian_percent'] = (philly_demographics['Asian'] / philly_demographics['Total']) * 100\nphilly_demographics['AI/AN'] = pd.to_numeric(philly_demographics['AI/AN'])\nphilly_demographics['NH/PI'] = pd.to_numeric(philly_demographics['NH/PI'])\nphilly_demographics['Other_'] = pd.to_numeric(philly_demographics['Other_'])\nphilly_demographics['Two Plus'] = pd.to_numeric(philly_demographics['Two Plus'])\ncolumns_to_sum = [\"AI/AN\", \"NH/PI\", \"Other_\", \"Two Plus\"]\nphilly_demographics['other_percent'] = (philly_demographics[columns_to_sum].sum(axis=1) / philly_demographics['Total']) * 100\nphilly_demographics = philly_demographics[['blk_percent','white_percent','latino_percent', 'asian_percent', 'other_percent', 'geoid']] \n\n\n\n\nCode\nphilly_demographics['geoid'] = philly_demographics['geoid'].astype('int64')\nphilly_demographics = philly_demographics.dropna()"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#correlation-plots",
    "href": "Our Model/1-Exploratory-Analysis.html#correlation-plots",
    "title": "Exploratory Analysis",
    "section": "Correlation Plots",
    "text": "Correlation Plots\nBelow, asthma risk facors and the CDC’s estimate of asthma prevalence within a census tract. In our analysis, we observed the most notable correlations with predictors such as the percentage of Black population in a census tract, indicating potential disparities. Prevalence of smoking appeared to have a positive correlation with the dependent variable. Furthermore, the health and wellness value appears to have a negative correlation with asthma prevalence in a given census tract.\n\n\nCode\nregression_df = pd.merge(health_and_wellness, ozone_concentration, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, pmc, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, CDC_data, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, eji_pa, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, philly_demographics, on='geoid', how='inner')\n\n\n\n\nCode\nregression_df.to_csv('regression_df.csv', index=False)\nregression_df.head()\n\n\n\n\n\n\n\n\n\ngeoid\nhw_value\noc_value\npmc_value\nasthma_prevalance\nsmoking_prevalance\nE_PARK\nE_HOUAGE\nE_ROAD\nblk_percent\nwhite_percent\nlatino_percent\nasian_percent\nother_percent\n\n\n\n\n0\n42101000200\n74.596253\n43.920837\n9.267069\n8.8\n14.8\n100.0\n68.17\n100.0\n5.131698\n14.577657\n7.402361\n65.667575\n7.220708\n\n\n1\n42101000200\n74.596253\n43.920837\n9.267069\n8.8\n14.8\n100.0\n68.17\n100.0\n4.251386\n55.452865\n0.000000\n39.279113\n1.016636\n\n\n2\n42101000200\n74.596253\n43.920837\n9.267069\n8.8\n14.8\n100.0\n68.17\n100.0\n41.538462\n39.230769\n6.153846\n13.076923\n0.000000\n\n\n3\n42101000300\n74.249221\n43.472629\n8.443596\n9.9\n8.6\n100.0\n63.11\n100.0\n2.544333\n60.061681\n5.551272\n12.181958\n19.660756\n\n\n4\n42101000300\n74.249221\n43.472629\n8.443596\n9.9\n8.6\n100.0\n63.11\n100.0\n3.191489\n63.829787\n0.000000\n12.234043\n20.744681\n\n\n\n\n\n\n\n\n\nCode\ndef plot_regression(x_variable, y_variable):\n    plt.figure(figsize=(8, 6))\n    \n    # Plot regression plot\n    sns.regplot(x=regression_df[x_variable], y=regression_df[y_variable])\n    \n    plt.title(f'Regression Plot - Asthma Prevalance vs. {x_variable}')\n    plt.xlabel('Predictor')\n    plt.ylabel('Asthma Prevalance')\n    plt.show()\n\n# List of variables in the dataset excluding 'asthma_prevalance' and 'geoid'\nvariables = [col for col in regression_df.columns if col not in [\"asthma_prevalance\", \"geoid\"]]\ny_variabless = [col for col in regression_df.columns if col in [\"asthma_prevalance\"]]\n\n# Create interactive widgets for x-axis and y-axis variables\nx_variable_dropdown = widgets.Dropdown(options=variables, description='X-axis Variable')\ny_variable_dropdown = widgets.Dropdown(options=y_variabless, description='Y-axis Variable')\n\n# Use the interact function to link the widgets to the plot function\ninteract(plot_regression, x_variable=x_variable_dropdown, y_variable=y_variable_dropdown);"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#risk-factor-choropleth-maps",
    "href": "Our Model/1-Exploratory-Analysis.html#risk-factor-choropleth-maps",
    "title": "Exploratory Analysis",
    "section": "Risk Factor Choropleth Maps",
    "text": "Risk Factor Choropleth Maps\nLastly, we investigate how these predictors look spatially within the city of Philadelphia. The dashboard below displays each predictor for asthma prevalence by census tract.\n\n\nCode\nregression_gdf = pd.merge(regression_df, censustracts, on='geoid', how='inner')\nregression_gdf = gpd.GeoDataFrame(regression_gdf, geometry='geometry')\n\n\n\n\nCode\ndef generate_choropleth(variable):\n    choropleth_map = regression_gdf.hvplot(geo=True, c=variable, cmap='viridis', colorbar=True, width=800, height=600)\n    return choropleth_map\n\nvariables = [col for col in regression_gdf.columns if col not in [\"geoid\", \"geometry\"]]\n\n# Create a widget for variable selection\nvariable_dropdown = pn.widgets.Select(options=variables, value=variables[0], name='Select Variable')\n\n# Use interact from panel to generate the choropleth map based on the selected variable\n@pn.depends(variable_dropdown.param.value)\ndef update_choropleth(variable):\n    return generate_choropleth(variable)\n\n# Combine the variable dropdown and the choropleth map\ndashboard = pn.Column(\n    pn.Row(variable_dropdown),\n    pn.Row(update_choropleth)\n)\n\ndashboard"
  },
  {
    "objectID": "Our Model/Predictions.html",
    "href": "Our Model/Predictions.html",
    "title": "Predicting Asthma Prevalence by Census Tracts in Philadeplhia, PA",
    "section": "",
    "text": "# Predictions"
  }
]