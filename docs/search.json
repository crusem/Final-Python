[
  {
    "objectID": "Our Model/About1.html",
    "href": "Our Model/About1.html",
    "title": "Our Model",
    "section": "",
    "text": "Our Model\nOur model uses a series of risk factors to build a machine learning algorithm to predict asthma prevalence by census tract in Philadelphia. Our process is outlined in the exploratory analysis, regression and cross validation, and prediction sections of this project."
  },
  {
    "objectID": "Our Model/2-Regression-Analysis.html",
    "href": "Our Model/2-Regression-Analysis.html",
    "title": "Regression Analysis & Cross-Validation",
    "section": "",
    "text": "C:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):\n## Plotting A Correlation Matrix\nCode\nregression_df = pd.read_csv('regression_df.csv')\nCode\nmodel = LinearRegression()\nmodel\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\nCode\nX = regression_df[['hw_value', 'oc_value', 'pmc_value', 'E_PARK', 'E_HOUAGE', 'smoking_prevalance','E_ROAD','blk_percent','white_percent','latino_percent', 'asian_percent', 'other_percent']].values\ny = regression_df['asthma_prevalance'].values\nCode\nfeature_cols = [col for col in regression_df.columns if col not in [\"asthma_prevalance\", \"geoid\"]]\ntrain_set, test_set = train_test_split(regression_df, test_size=0.3, random_state=42)\ny_train = train_set[\"asthma_prevalance\"].values\ny_test = test_set[\"asthma_prevalance\"].values\nX_train = train_set[feature_cols].values\nX_test = test_set[feature_cols].values"
  },
  {
    "objectID": "Our Model/2-Regression-Analysis.html#correlation-matrix",
    "href": "Our Model/2-Regression-Analysis.html#correlation-matrix",
    "title": "Regression Analysis & Cross-Validation",
    "section": "Correlation Matrix",
    "text": "Correlation Matrix\nUsing the seaborn package in python, we plot a correlation matrix for each of our predicotrs to observe relationships between different variables. From this matrix, we are able to see variables like ozone concentration and particle matter concentration are correlated. The smoking prevalence and health and wellness values are also correlated with one another. Lastly, we are able to see some disparities with this correlation matrix. ‘blk_percent’ and ‘latino_percent’ have a positive correlation with smoking prevalence and a negative correlation with health and wellness, indicating that a higher percentage of Black or Latino population in a given census tract is associated with a higher prevalence of smoking and lower health and wellness value.\n\n\nCode\nsns.heatmap(\n    train_set[feature_cols].corr(), \n    cmap=\"coolwarm\", \n    annot=True, \n    vmin=-1, \n    vmax=1\n);\n\n\nNameError: name 'sns' is not defined\n\n\n\n\nCode\nX.shape\n\n\n(1179, 12)\n\n\n\n\nCode\ny.shape\n\n\n(1179,)"
  },
  {
    "objectID": "Our Model/2-Regression-Analysis.html#running-our-regressions",
    "href": "Our Model/2-Regression-Analysis.html#running-our-regressions",
    "title": "Regression Analysis & Cross-Validation",
    "section": "Running Our Regressions",
    "text": "Running Our Regressions\nWe want to use the best model for our data. So we use the ‘scikit-learn’ package to conduct and compare a linear regression model and a Random Forest Model. Linear regressions assume linear relationships between variables and is suitable when the relationship between input features and the dependent variable can be described linearly. The Random Forest Model is able to capture more complex relationships within the dataset and is robust against overfitting.\n\nLinear Regression - R- Squared\nUsing the ‘scikit-learn’ library in Python, we performed a linear regression with feature scaling using a pipeline. To assess the model’s predictive performance, we evalute the R-Square values on both the training and test datasets. We will compare these R-Squared values to the R-Squared values from our Random Forest Model to decide which model to use for predictions.\n\n\nCode\nmodel.fit(X, y)\n\n\nLinearRegression()In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.LinearRegressionLinearRegression()\n\n\n\n\n0.8813734475169395\n\n\n\n\nCode\nlinear_pipe = make_pipeline(StandardScaler(), LinearRegression())\n\nprint(\"Linear regression\")\nlinear_pipe.fit(X_train, y_train)\n\ntraining_score = linear_pipe.score(X_train, y_train)\nprint(f\"Training Score = {training_score}\")\n\ntest_score = linear_pipe.score(X_test, y_test)\nprint(f\"Test Score = {test_score}\")\n\n\nLinear regression\nTraining Score = 0.8771018545312711\nTest Score = 0.889628138385183\n\n\n\n\nRandom Forest Model and Important Features - R-Squared\nUsing the ‘scikit-learn’ library in Python, we built a Random Forest Model with feature scaling using a pipeline. To assess the model’s predictive performance, we evalute the R-Square values on both the training and test datasets.\n\n\nCode\nforest_pipe = make_pipeline(\n    StandardScaler(),  # Pre-process step\n    RandomForestRegressor(n_estimators=100, max_depth=2, random_state=42),  # Model step\n)\n\nprint(\"Random forest\")\nforest_pipe.fit(X_train, y_train)\n\ntraining_score = forest_pipe.score(X_train, y_train)\nprint(f\"Training Score = {training_score}\")\n\ntest_score = forest_pipe.score(X_test, y_test)\nprint(f\"Test Score = {test_score}\")\n\n\nRandom forest\nTraining Score = 0.802226720882675\nTest Score = 0.7982253593098484\n\n\nBased off of the R-squared values, it seems as though the linear regression model may do a better job at predicting variability in the dependent variable.\n\n\narray([0.13976872, 0.        , 0.        , 0.3803543 , 0.        ,\n       0.        , 0.        , 0.04474563, 0.43513135, 0.        ,\n       0.        , 0.        ])\n\n\n\n\nCode\nimportance = pd.DataFrame(\n    {\"Feature\": feature_cols, \"Importance\": forest_model.feature_importances_}\n).sort_values(\"Importance\", ascending=False)\n\n\n\n\nRandom Forest Model - Feature Importance\nThe random forest model assesses the contribution of each feature in predicting the target variable. This information helps us understand which features have the most influence on the model’s prediction and gain insights to how we can improve the model’s performance in the future. From Our Feature Importance table, were able to see that features such as percent of White population, smoking prevalence, health and wellness value, and percent of Black population within a given census tract in Philadelphia are the most important features in our Random Forest Model.\n\n\nCode\nimportance\n\n\n\n\n\n\n\n\n\nFeature\nImportance\n\n\n\n\n8\nwhite_percent\n0.435131\n\n\n3\nsmoking_prevalance\n0.380354\n\n\n0\nhw_value\n0.139769\n\n\n7\nblk_percent\n0.044746\n\n\n1\noc_value\n0.000000\n\n\n2\npmc_value\n0.000000\n\n\n4\nE_PARK\n0.000000\n\n\n5\nE_HOUAGE\n0.000000\n\n\n6\nE_ROAD\n0.000000\n\n\n9\nlatino_percent\n0.000000\n\n\n10\nasian_percent\n0.000000\n\n\n11\nother_percent\n0.000000\n\n\n\n\n\n\n\n\n\nCode\nimport hvplot.pandas\nimportance.sort_values(\"Importance\", ascending=True).hvplot.barh(\n    x=\"Feature\", y=\"Importance\", title=\"Factors that Impact Asthma Prevalance\"\n)\n\n\n\n\n\n\n  \n\n\n\n\n\n\nCode\nmodel = linear_pipe['linearregression']"
  },
  {
    "objectID": "Our Model/2-Regression-Analysis.html#cross-validation",
    "href": "Our Model/2-Regression-Analysis.html#cross-validation",
    "title": "Regression Analysis & Cross-Validation",
    "section": "Cross-Validation",
    "text": "Cross-Validation\nFor each model, we conduct a 3 fold cross-validation for both of our models. The model is trained three time, each time using a different fold as the test set and the remaining folds for training. The R-squared scores are computed for each fold. Higher R-Squared values indicate a better predictive performance. We will select the model with the highest average R-Squared across folds.\n\nLinear Regression - Cross Validation\nThe results from our 3-fold cross-validation for a linear regression model are presented below.\n\n\nCode\nlinear_pipe = make_pipeline(StandardScaler(), LinearRegression())\n\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    linear_pipe,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.87672721 0.86979481 0.86632107]\nScores mean =  0.8709476945361351\nScore std dev =  0.004325797875727253\n\n\n\n\nRandom Forest Regression - Cross Validation\nThe results from our 3-fold cross-validation for a linear regression model are presented below. Based off of the average R-squared across folds, this model has a better predictive performance for our data. We will use this model to make predictions for asthma prevalence.\n\n\nCode\nforest_pipe = make_pipeline(\n    StandardScaler(), RandomForestRegressor(n_estimators=100, random_state=42)\n)\n\n# Run the 3-fold cross validation\nscores = cross_val_score(\n    forest_pipe,\n    X_train,\n    y_train,\n    cv=3,\n)\n\n# Report\nprint(\"R^2 scores = \", scores)\nprint(\"Scores mean = \", scores.mean())\nprint(\"Score std dev = \", scores.std())\n\n\nR^2 scores =  [0.8692119  0.91654952 0.9188434 ]\nScores mean =  0.9015349385674881\nScore std dev =  0.022875019221878082"
  },
  {
    "objectID": "Our Model/2-Regression-Analysis.html#model-performance",
    "href": "Our Model/2-Regression-Analysis.html#model-performance",
    "title": "Regression Analysis & Cross-Validation",
    "section": "Model Performance",
    "text": "Model Performance\nAfter using the random forest model to predict asthma values, we assess our model’s predictive capabilities to calculate our mean percent error.\n\n\nPipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\n{'standardscaler': StandardScaler(),\n 'randomforestregressor': RandomForestRegressor(random_state=42)}\n\n\n\n\n{'randomforestregressor__n_estimators': [5, 10, 15, 20, 30, 50, 100, 200],\n 'randomforestregressor__max_depth': [2, 5, 7, 9, 13, 21, 33, 51]}\n\n\n\n\nFitting 3 folds for each of 64 candidates, totalling 192 fits\n\n\nGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=3,\n             estimator=Pipeline(steps=[('standardscaler', StandardScaler()),\n                                       ('randomforestregressor',\n                                        RandomForestRegressor(random_state=42))]),\n             param_grid={'randomforestregressor__max_depth': [2, 5, 7, 9, 13,\n                                                              21, 33, 51],\n                         'randomforestregressor__n_estimators': [5, 10, 15, 20,\n                                                                 30, 50, 100,\n                                                                 200]},\n             verbose=1)estimator: PipelinePipeline(steps=[('standardscaler', StandardScaler()),\n                ('randomforestregressor',\n                 RandomForestRegressor(random_state=42))])StandardScalerStandardScaler()RandomForestRegressorRandomForestRegressor(random_state=42)\n\n\n\n\nCode\ndef evaluate_mape(model, X_test, y_test):\n    \"\"\"\n    Given a model and test features/targets, print out the \n    mean absolute error and accuracy\n    \"\"\"\n    # Make the predictions\n    predictions = model.predict(X_test)\n\n    # Absolute error\n    errors = abs(predictions - y_test)\n    avg_error = np.mean(errors)\n\n    # Mean absolute percentage error\n    mape = 100 * np.mean(errors / y_test)\n\n    # Accuracy\n    accuracy = 100 - mape\n\n    print(\"Model Performance\")\n    print(f\"Average Absolute Error: {avg_error:0.4f}\")\n    print(f\"Accuracy = {accuracy:0.2f}%.\")\n\n    return accuracy\n\n\nOur model boasts an accuracy of 97.68%, suggesting that our model’s prediction closely align with the values provided by teh CDC.\n\n\nCode\nbase_model = make_pipeline(StandardScaler(), RandomForestRegressor(random_state=42))\n\n# Fit the training set\nbase_model.fit(X_train, y_train)\n\n# Evaluate on the test set\nbase_accuracy = evaluate_mape(base_model, X_test, y_test)\n\n\nModel Performance\nAverage Absolute Error: 0.2733\nAccuracy = 97.68%.\n\n\n\n\nCode\ndata = regression_df.loc[test_set.index]\n\n\n\ncensustracts = gpd.read_file('Census_Tracts_2010.geojson')\ntract_and_geoid = censustracts[['GEOID10', 'TRACTCE10']]\ntract_and_geoid = tract_and_geoid.rename(columns = {'TRACTCE10':'tract'})\n\ncensustracts = censustracts[['GEOID10', 'geometry']]\ncensustracts = censustracts.rename(columns = {'GEOID10':'geoid'})\ncensustracts['geoid'] = censustracts['geoid'].astype(np.int64)\n\n\n\nCode\ndata['prediction'] = base_model.predict(X_test)\ndata.to_csv('data.csv', index=False)\n\n\n\n\nCode\ndata = pd.merge(data, censustracts, on='geoid', how ='inner')\n\n\n\n\nCode\ndata = gpd.GeoDataFrame(data, geometry = 'geometry')"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Predicting Asthma Prevalence by Census Tracts in Philadeplhia, PA",
    "section": "",
    "text": "Asthma, a chronic lung disease, is largely impacted by a combination of genetics and social determinants of health, defined as conditions in the environments that people are born and live in that affect health and quality of life outcomes [1]. Within the Philadelphia region, there are a combination of both indoor and outdoor factors that contribute to a high prevalence of asthma. In understanding this, according to the Center for Disease Control and Prevention(CDC), 5.8 percent of children in the United States have asthma, while 21 percent of children in Philadelphia have asthma [2]. Philadelphia was ranked the seventh most challenging place to live with asthma, where the burden is high among racial and ethnic groups, where Black individuals are approximately three times more likely to die from asthma than white individuals [2]. With this context, our analysis builds a random forest regression model in order to predict asthma prevalence by census tract in the City of Philadelphia, validating our model against the estimated asthma prevalence from the CDC. In conducting a literature review, our prediction model identified variables/predictors that lead to disproportionate rates of asthma in the City of Philadelphia region. These predictors originate from the Environmental Justice Index [3], CDC’s PLACES[4] and Philadelphia’s Social Progress Index[5]."
  },
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Predicting Asthma Prevalence by Census Tracts in Philadeplhia, PA",
    "section": "",
    "text": "Asthma, a chronic lung disease, is largely impacted by a combination of genetics and social determinants of health, defined as conditions in the environments that people are born and live in that affect health and quality of life outcomes [1]. Within the Philadelphia region, there are a combination of both indoor and outdoor factors that contribute to a high prevalence of asthma. In understanding this, according to the Center for Disease Control and Prevention(CDC), 5.8 percent of children in the United States have asthma, while 21 percent of children in Philadelphia have asthma [2]. Philadelphia was ranked the seventh most challenging place to live with asthma, where the burden is high among racial and ethnic groups, where Black individuals are approximately three times more likely to die from asthma than white individuals [2]. With this context, our analysis builds a random forest regression model in order to predict asthma prevalence by census tract in the City of Philadelphia, validating our model against the estimated asthma prevalence from the CDC. In conducting a literature review, our prediction model identified variables/predictors that lead to disproportionate rates of asthma in the City of Philadelphia region. These predictors originate from the Environmental Justice Index [3], CDC’s PLACES[4] and Philadelphia’s Social Progress Index[5]."
  },
  {
    "objectID": "index.html#about-the-authors",
    "href": "index.html#about-the-authors",
    "title": "Predicting Asthma Prevalence by Census Tracts in Philadeplhia, PA",
    "section": "About the Authors",
    "text": "About the Authors\nAlyssa Felix-Arreola, B.S. in Global Health from the University of Southern California, and Marissa Cruse, B.S. in Economics from the University of Nebraska-Lincoln completed this project as our final assignment for Geospatial Data Science In Python, a course taught by Nick Hand at the University of Pennsylvania in the fall semester of 2023."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About me",
    "section": "",
    "text": "On this about page, you might want to add more information about yourself, the project, or course. Any helpful context could go here!\nMy name is Nick Hand, the instructor for the course. You can find more information about me on my personal website.\nThis site is an example site showing how to use Quarto for the final project for MUSA 550, during fall 2023.\nAdipisicing proident minim non non dolor quis. Pariatur in ipsum aliquip magna. Qui ad aliqua nulla excepteur dolor nostrud quis nisi. Occaecat proident eiusmod in cupidatat. Elit qui laboris sit aliquip proident dolore. Officia commodo commodo in eiusmod aliqua sint cupidatat consectetur aliqua sint reprehenderit.\nOccaecat incididunt esse et elit adipisicing sit est cupidatat consequat. Incididunt exercitation amet dolor non sit anim veniam veniam sint velit. Labore irure reprehenderit ut esse. Minim quis commodo nisi voluptate."
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html",
    "href": "Our Model/1-Exploratory-Analysis.html",
    "title": "Exploratory Analysis",
    "section": "",
    "text": "C:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:39: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_dist(x, y):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:165: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def get_faces(triangle):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:199: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def build_faces(faces, triangles_is, num_triangles, num_faces_single):\nC:\\Users\\cruse\\mambaforge1\\envs\\musa-550-fall-2023\\lib\\site-packages\\libpysal\\cg\\alpha_shapes.py:261: NumbaDeprecationWarning: The 'nopython' keyword argument was not supplied to the 'numba.jit' decorator. The implicit default value for this argument is currently False, but it will be changed to True in Numba 0.59.0. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n  def nb_mask_faces(mask, faces):"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#philadelphia-social-progress-index-data",
    "href": "Our Model/1-Exploratory-Analysis.html#philadelphia-social-progress-index-data",
    "title": "Exploratory Analysis",
    "section": "Philadelphia Social Progress Index Data",
    "text": "Philadelphia Social Progress Index Data\nAs social determinants of health influence the overall health of individuals, including asthma rates Philadelphia’s Social Progress Index, which measures social progress using a detailed framework of indicators applied in 372 census tracts, helps account for indicators related to conditions of environments that Philadelphians grew up in [5]. This analysis utilizes Health and Wellness data, represented as hw_value, to understand the overall health of communities, as more Black and Hispanic residents reported poor or fair health than any other group, with severe declines in life expectancy [6]. Philadelphia ranked among the worst 25 in the nation for ozone and particle pollution in the country, which are related to serious health effects such as asthma attacks. Further, particle pollution has been linked to the development of asthma [7]. To increase the accuracy of our prediction model, we used Ozone and Particulate Matter 2.5 data, represented as oc_value and pmc_value, respectively.\n\n\nCode\nhealth_and_wellness = pd.read_csv(\"health_and_wellness.csv\")\nhealth_and_wellness = health_and_wellness.drop(['rank', 'region_name', 'tract_name', 'variable', 'neighborhood_name', 'average_label'], axis=1)\nhealth_and_wellness = health_and_wellness.rename(columns={'value':'hw_value'})\n\n\n\n\nCode\nozone_concentration = pd.read_csv(\"ozone_concentration.csv\")\nozone_concentration = ozone_concentration.drop(['rank', 'region_name', 'tract_name', 'variable','neighborhood_name', 'average_label'], axis=1)\nozone_concentration = ozone_concentration.rename(columns={'value':'oc_value'})\n\n\n\n\nCode\npmc = pd.read_csv(\"particular_matter_concentration.csv\")\npmc = pmc.drop(['rank', 'region_name', 'tract_name', 'variable','neighborhood_name', 'average_label'], axis=1)\npmc = pmc.rename(columns={'value':'pmc_value'})"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#cdc-places-data",
    "href": "Our Model/1-Exploratory-Analysis.html#cdc-places-data",
    "title": "Exploratory Analysis",
    "section": "CDC PLACES Data",
    "text": "CDC PLACES Data\nThis analysis builds off of the CDC’s PLACES dataset, a model-based population-level analysis and community estimates of health measures across the U.S [4]. Our model uses asthma prevalence data, represented as asthma_prevalance, from this dataset as our dependent variable. Tobacco Smoke, represented through the smoking prevalence data, smoking_prevalance, is known to be an indoor and outdoor pollution source that triggers asthma. In the City of Philadelphia, while the rates are declining, Philadelphians continue to have the highest smoking rate among large U.S cities, with non-Hispanic Black and Hispanic residents more likely to report smoking [6].\n\n\nCode\nCDC_data = pd.read_csv(\"CDC_data.csv\")\nCDC_data = CDC_data[(CDC_data['StateAbbr'] == 'PA') &\n                         (CDC_data['StateDesc'] == 'Pennsylvania') &\n                         (CDC_data['CountyName'] == 'Philadelphia')]\nCDC_data = CDC_data[['CASTHMA_CrudePrev', 'TractFIPS', 'CSMOKING_CrudePrev']]\nCDC_data = CDC_data.rename(columns={'TractFIPS':'geoid'})\nCDC_data = CDC_data.rename(columns={'CASTHMA_CrudePrev':'asthma_prevalance'})\nCDC_data = CDC_data.rename(columns={'CSMOKING_CrudePrev': 'smoking_prevalance'})"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#environmental-justice-index-data",
    "href": "Our Model/1-Exploratory-Analysis.html#environmental-justice-index-data",
    "title": "Exploratory Analysis",
    "section": "Environmental Justice Index Data",
    "text": "Environmental Justice Index Data\nThis analysis uses data from the Environmental Justice Index, a place-based tool that is designed to measure cumulative impacts of environmental burden that affects health and health equity [3]. In Philadelphia, children are largely exposed to lead, mold and tobacco smoke in the homes of Philadelphia’s residents, which are common asthma triggers [8]. The predictive model reflects these indoor exposures, by including the percentage of houses built pre-1980, as nearly 90 percent of homes were built before 1978, represented as e_houage[8]. The Center of Excellence in Environmental Toxicology shares that children living close to busy roadways is also a major concern for asthma, justifying the data for the proportion of tract’s area within 1 mile buffer of a high-volume road or highway, e_road [8]. Green spaces remain a protective factor that improves air quality, including ozone and particle pollution, and increases the health of individuals, demonstrated in as a predictor variable, e_park, that provides the proportion of the tract’s area within a 1 mile buffer of greenspace[9].\n\n\nCode\neji_pa = pd.read_csv(\"eji_pa.csv\")\neji_pa = eji_pa[(eji_pa['COUNTY'] == 'Philadelphia')]\neji_pa = eji_pa[['GEOID', 'E_PARK', 'E_HOUAGE','E_ROAD']]\neji_pa = eji_pa.rename(columns={'GEOID': 'geoid'})\n\n\n\n\nCode\ncensustracts = gpd.read_file('Census_Tracts_2010.geojson')\ntract_and_geoid = censustracts[['GEOID10', 'TRACTCE10']]\ntract_and_geoid = tract_and_geoid.rename(columns = {'TRACTCE10':'tract'})\ncensustracts = censustracts[['GEOID10', 'geometry']]\ncensustracts = censustracts.rename(columns = {'GEOID10':'geoid'})\ncensustracts['geoid'] = censustracts['geoid'].astype(np.int64)"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#philly-demographics",
    "href": "Our Model/1-Exploratory-Analysis.html#philly-demographics",
    "title": "Exploratory Analysis",
    "section": "Philly Demographics",
    "text": "Philly Demographics\nAs Black and Latino individuals encounter higher asthma burden, our prediction model uses the Census Bureau to include demographic data including: the percentage of Black, Latino, Asian and other races and ethnicities in Philadelphia. This is represented as blk_percent, white_percent, latino_percent, asian_percent, and other_percent.\n\n\nCode\nvariables = [\n    \"NAME\",\n    \"B03002_001E\",\n    \"B03002_003E\", \n    \"B03002_004E\", \n    \"B03002_005E\", \n    \"B03002_006E\", \n    \"B03002_007E\", \n    \"B03002_008E\", \n    \"B03002_009E\", \n    \"B03002_012E\", \n]\n\n\n\n\nCode\navailable = cenpy.explorer.available()\nacs = cenpy.remote.APIConnection(\"ACSDT5Y2021\")\nphilly_county_code = \"101\"\npa_state_code = \"42\"\n\n\n\n\nCode\nphilly_demographics = acs.query(\n    cols=variables,\n    geo_unit=\"block group:*\",\n    geo_filter={\"state\": pa_state_code, \"county\": philly_county_code, \"tract\": \"*\"},\n)\n\nphilly_demographics = philly_demographics.rename(\n    columns={\n        \"B03002_001E\": \"Total\", \n        \"B03002_003E\": \"White\",  \n        \"B03002_004E\": \"Black\",  \n        \"B03002_005E\": \"AI/AN\", \n        \"B03002_006E\": \"Asian\",  \n        \"B03002_007E\": \"NH/PI\", \n        \"B03002_008E\": \"Other_\",  \n        \"B03002_009E\": \"Two Plus\",\n        \"B03002_012E\": \"Hispanic\",\n    }\n)\nphilly_demographics = pd.merge(philly_demographics, tract_and_geoid, on='tract', how='inner')\nphilly_demographics = philly_demographics.rename(columns = {'GEOID10':'geoid'})\n\n\n\n\nCode\nphilly_demographics['Black'] = pd.to_numeric(philly_demographics['Black'])\nphilly_demographics['Total'] = pd.to_numeric(philly_demographics['Total'])\nphilly_demographics['blk_percent'] = (philly_demographics['Black'] / philly_demographics['Total']) * 100\n\n\n\n\nCode\nphilly_demographics['White'] = pd.to_numeric(philly_demographics['White'])\nphilly_demographics['white_percent'] = (philly_demographics['White'] / philly_demographics['Total']) * 100\nphilly_demographics['Hispanic'] = pd.to_numeric(philly_demographics['Hispanic'])\nphilly_demographics['latino_percent'] = (philly_demographics['Hispanic'] / philly_demographics['Total']) * 100\nphilly_demographics['Asian'] = pd.to_numeric(philly_demographics['Asian'])\nphilly_demographics['asian_percent'] = (philly_demographics['Asian'] / philly_demographics['Total']) * 100\nphilly_demographics['AI/AN'] = pd.to_numeric(philly_demographics['AI/AN'])\nphilly_demographics['NH/PI'] = pd.to_numeric(philly_demographics['NH/PI'])\nphilly_demographics['Other_'] = pd.to_numeric(philly_demographics['Other_'])\nphilly_demographics['Two Plus'] = pd.to_numeric(philly_demographics['Two Plus'])\ncolumns_to_sum = [\"AI/AN\", \"NH/PI\", \"Other_\", \"Two Plus\"]\nphilly_demographics['other_percent'] = (philly_demographics[columns_to_sum].sum(axis=1) / philly_demographics['Total']) * 100\nphilly_demographics = philly_demographics[['blk_percent','white_percent','latino_percent', 'asian_percent', 'other_percent', 'geoid']] \n\n\n\n\nCode\nphilly_demographics['geoid'] = philly_demographics['geoid'].astype('int64')\nphilly_demographics = philly_demographics.dropna()"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#correlation-plots",
    "href": "Our Model/1-Exploratory-Analysis.html#correlation-plots",
    "title": "Exploratory Analysis",
    "section": "Correlation Plots",
    "text": "Correlation Plots\nBelow, asthma risk facors and the CDC’s estimate of asthma prevalence within a census tract. In our analysis, we observed the most notable correlations with predictors such as the percentage of Black population in a census tract, indicating potential disparities. Prevalence of smoking appeared to have a positive correlation with the dependent variable. Furthermore, the health and wellness value appears to have a negative correlation with asthma prevalence in a given census tract.\n\n\nCode\nregression_df = pd.merge(health_and_wellness, ozone_concentration, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, pmc, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, CDC_data, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, eji_pa, on='geoid', how='inner')\nregression_df = pd.merge(regression_df, philly_demographics, on='geoid', how='inner')\n\n\n\n\nCode\nregression_df.to_csv('regression_df.csv', index=False)\nregression_df.head()\n\n\n\n\n\n\n\n\n\ngeoid\nhw_value\noc_value\npmc_value\nasthma_prevalance\nsmoking_prevalance\nE_PARK\nE_HOUAGE\nE_ROAD\nblk_percent\nwhite_percent\nlatino_percent\nasian_percent\nother_percent\n\n\n\n\n0\n42101000200\n74.596253\n43.920837\n9.267069\n8.8\n14.8\n100.0\n68.17\n100.0\n5.131698\n14.577657\n7.402361\n65.667575\n7.220708\n\n\n1\n42101000200\n74.596253\n43.920837\n9.267069\n8.8\n14.8\n100.0\n68.17\n100.0\n4.251386\n55.452865\n0.000000\n39.279113\n1.016636\n\n\n2\n42101000200\n74.596253\n43.920837\n9.267069\n8.8\n14.8\n100.0\n68.17\n100.0\n41.538462\n39.230769\n6.153846\n13.076923\n0.000000\n\n\n3\n42101000300\n74.249221\n43.472629\n8.443596\n9.9\n8.6\n100.0\n63.11\n100.0\n2.544333\n60.061681\n5.551272\n12.181958\n19.660756\n\n\n4\n42101000300\n74.249221\n43.472629\n8.443596\n9.9\n8.6\n100.0\n63.11\n100.0\n3.191489\n63.829787\n0.000000\n12.234043\n20.744681\n\n\n\n\n\n\n\n\n\nCode\ndef plot_regression(x_variable, y_variable):\n    plt.figure(figsize=(8, 6))\n    \n    # Plot regression plot\n    sns.regplot(x=regression_df[x_variable], y=regression_df[y_variable])\n    \n    plt.title(f'Regression Plot - Asthma Prevalance vs. {x_variable}')\n    plt.xlabel('Predictor')\n    plt.ylabel('Asthma Prevalance')\n    plt.show()\n\n# List of variables in the dataset excluding 'asthma_prevalance' and 'geoid'\nvariables = [col for col in regression_df.columns if col not in [\"asthma_prevalance\", \"geoid\"]]\ny_variabless = [col for col in regression_df.columns if col in [\"asthma_prevalance\"]]\n\n# Create interactive widgets for x-axis and y-axis variables\nx_variable_dropdown = widgets.Dropdown(options=variables, description='X-axis Variable')\ny_variable_dropdown = widgets.Dropdown(options=y_variabless, description='Y-axis Variable')\n\n# Use the interact function to link the widgets to the plot function\ndashboard1 = interact(plot_regression, x_variable=x_variable_dropdown, y_variable=y_variable_dropdown);\ndisplay(dashboard1)\n\n\n\n\n\n&lt;function __main__.plot_regression(x_variable, y_variable)&gt;"
  },
  {
    "objectID": "Our Model/1-Exploratory-Analysis.html#risk-factor-choropleth-maps",
    "href": "Our Model/1-Exploratory-Analysis.html#risk-factor-choropleth-maps",
    "title": "Exploratory Analysis",
    "section": "Risk Factor Choropleth Maps",
    "text": "Risk Factor Choropleth Maps\nLastly, we investigate how these predictors look spatially within the city of Philadelphia. The dashboard below displays each predictor for asthma prevalence by census tract.\n\n\nCode\nregression_gdf = pd.merge(regression_df, censustracts, on='geoid', how='inner')\nregression_gdf = gpd.GeoDataFrame(regression_gdf, geometry='geometry')\n\n\n\n\nCode\ndef generate_choropleth(variable):\n    choropleth_map = regression_gdf.hvplot(geo=True, c=variable, cmap='viridis', colorbar=True, width=800, height=600)\n    return choropleth_map\n\nvariables = [col for col in regression_gdf.columns if col not in [\"geoid\", \"geometry\"]]\n\n# Create a widget for variable selection\nvariable_dropdown = pn.widgets.Select(options=variables, value=variables[0], name='Select Variable')\n\n# Use interact from panel to generate the choropleth map based on the selected variable\n@pn.depends(variable_dropdown.param.value)\ndef update_choropleth(variable):\n    return generate_choropleth(variable)\n\n# Combine the variable dropdown and the choropleth map\ndashboard = pn.Column(\n    pn.Row(variable_dropdown),\n    pn.Row(update_choropleth)\n)\n\ndashboard"
  },
  {
    "objectID": "Our Model/3-Predictions.html",
    "href": "Our Model/3-Predictions.html",
    "title": "Predictions",
    "section": "",
    "text": "We plot our predicted asthma prevalence against CDC asthma prevalence predictions per census tract from our test data. From this vizualization, you are able to see our model’s predictive capabilities.\n\n\nCode\nfig, axs = plt.subplots(ncols=2, figsize=(10, 10))\n\n# Predicted values\ndata.plot(\n    ax=axs[0],\n    column='prediction',\n    legend=True,\n    cmap='viridis',\n    linewidth=0.8,\n    edgecolor='0.8',\n    legend_kwds={'label': \"Predicted Asthma Prevalence\", 'orientation': \"horizontal\", 'shrink': 0.8}\n)\naxs[0].set_title(\"Predicted Asthma Prevalence\")\n\n# Actual values\ndata.plot(\n    ax=axs[1],\n    column='asthma_prevalance',\n    legend=True,\n    cmap='viridis',\n    linewidth=0.8,\n    edgecolor='0.8',\n    legend_kwds={'label': \"Actual Asthma Prevalence\", 'orientation': \"horizontal\", 'shrink': 0.8}\n)\naxs[1].set_title(\"Actual Asthma Prevalence\")\n\naxs[0].set_axis_off()\naxs[1].set_axis_off()\n\nplt.show()"
  }
]